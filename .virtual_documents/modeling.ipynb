import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression


df = pd.read_csv('./data/cleaned_music.csv')


df.head()


X = df.drop(columns=['genre'])
y = df['genre']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)


scaler = StandardScaler()


X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)








knn = KNeighborsClassifier()


param_grid = {'n_neighbors': np.arange(3, 31, 2),
             'metric': ['minkowski', 'manhattan', 'cosine'],
             'weights': ['uniform', 'distance']}


# knn_grid = GridSearchCV(knn, param_grid, cv=kf)


# knn_grid.fit(X_train_scaled,y_train)


# knn_grid.best_score_


# knn_grid.best_params_


# knn_grid.score(X_train_scaled, y_train)


# knn_grid.score(X_test_scaled, y_test)





rf = RandomForestClassifier(random_state=42)


rf_param_grid = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', None]
}


# rf_grid = GridSearchCV(rf, rf_param_grid, cv=kf)


# rf_grid.fit(X_train_scaled, y_train)


# rf_grid.score(X_train_scaled, y_train)


# rf_grid.score(X_test_scaled, y_test)


# rf_grid.best_params_


import pickle


# with open('./best_models/rf.pkl', 'wb') as file:
#     pickle.dump(rf_grid.best_estimator_, file)


# with open('./best_models/knn.pkl', 'wb') as file:
#     pickle.dump(knn_grid.best_estimator_, file)





lr = LogisticRegression()


lr_param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'solver':['lbfgs', 'sag', 'saga']
}


lr_grid = GridSearchCV(lr, lr_param_grid, cv=kf)


# lr_grid.fit(X_train, y_train)


# lr_grid.best_score_


# lr_grid.score(X_test, y_test)


# lr_grid.best_params_


# with open('./best_models/lr.pkl', 'wb') as file:
#     pickle.dump(lr_grid.best_estimator_, file)





from sklearn.linear_model import RidgeClassifierCV


rc = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10, 100], cv=3, class_weight='balanced')


rc.fit(X_train, y_train)


rc.score(X_test, y_test)


from sklearn.model_selection import RandomizedSearchCV


from sklearn.ensemble import GradientBoostingClassifier


gbc_grid_params = {
    'n_estimators': np.arange(50, 300, 50),
    'n_iter_no_change': np.arange(5, 15, 1),
    'max_depth': [3, 4, 5]
}


# gbc = GradientBoostingClassifier(n_estimators=200, n_iter_no_change=10)
gbc = GradientBoostingClassifier(random_state=42)


gbc_grid = RandomizedSearchCV(gbc, gbc_grid_params, cv=3, n_iter=5)


# gbc_grid.fit(X_train_scaled, y_train)


# gbc_grid.score(X_test_scaled, y_test)


# gbc_grid.best_params_


# with open('./best_models/gbc.pkl', 'wb') as file:
#     pickle.dump(gbc_grid.best_estimator_, file)





knn =  pickle.load(open('./best_models/knn.pkl','rb'))
rf =  pickle.load(open('./best_models/rf.pkl','rb'))
lr =  pickle.load(open('./best_models/lr.pkl','rb'))


knn.score(X_test_scaled, y_test)


rf.score(X_test_scaled, y_test)


lr.score(X_test, y_test)











rf_preds = rf.predict(X_test_scaled)


from sklearn.metrics import classification_report, ConfusionMatrixDisplay


print(classification_report(y_test, rf_preds))


plt.figure(figsize=(10, 16))
ConfusionMatrixDisplay.from_estimator(rf, X_test_scaled, y_test, cmap='Blues')
plt.xticks(rotation=45);


# with open('./scaler.pkl', 'wb') as file:
#     pickle.dump(scaler, file)


X_test_scaled[0].reshape(1, -1)


X_test.head(1)


rf.predict(X_test_scaled[0].reshape(1, -1))


rf.predict_proba(X_test_scaled[0].reshape(1, -1))


rf.classes_











y_test.head()


X_test.head()


X_test.describe()



